\documentclass[11pt]{elegantbook}
\setlength{\parindent}{0pt}

\title{Paper-List}
\subtitle{Documenting my literature reading journey}
\author{Feiyu Zhou}
\institute{State Key Laboratory of Strength and Vibration of Mechanical Structures,
\\ College of Aeronautics and Astronautics, Xi'an Jiaotong University}
\bioinfo{School:}{Xi'an Jiaotong University}
\date{\today}
\bioinfo{Mail:}{zhoufy.xjtu@gmail.cn}
\logo{xjtu.png}
\cover{cover.png}
\extrainfo{Literature review summary focus : what is studied, where is the innovation point, what is the research method, what is the conclusion.

Research entry point : which methodologies have been employed to address the fundamental issue, what are the strengths and limitations of the approach, and how can it be enhanced.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\frontmatter
\tableofcontents


\mainmatter

\chapter{Transformer}
\section{Equiformer}

\subsection{What is an equivariant neural network?}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{2006.10503 (arxiv.org)}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
    \end{brief}
Let V and W be sets, and f : V → W a function. If a group G acts on both V and W, and this action commutes with the function $f: f(x · v) = x · f(v)$ for all $v \in V, x \in G$, then we say that f is \textbf{G-equivariant}.The special case where G acts trivially on W is called \textbf{G-invariant}.



\subsection{Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds
}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author:  Nathaniel Thomas(Stanford University), Tess Smidt(University of California)
    \item Artical address: \href{https://arxiv.org/abs/1802.08219}{https://arxiv.org/abs/1802.08219}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}



\subsection{SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{https://arxiv.org/pdf/2006.10503}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}


Check out \href{https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/}{this work} by Alexandre Milesi et al. from Nvidia. They managed to speed up training of the SE(3)-Transformer by up to 21(!) times and reduced memory consumption by up to 43 times. \href{https://github.com/NVIDIA/DeepLearningExamples/tree/master/DGLPyTorch/DrugDiscovery/SE3Transformer}{Code here}.

The SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations.





\subsection{Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs}
\begin{brief}
    \item Journal: Arxiv
    \item Artical address: \href{https://arxiv.org/pdf/2206.11990}{https://arxiv.org/pdf/2206.11990}
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Code address: \href{https://github.com/atomicarchitects/equiformer}{https://github.com/atomicarchitects/equiformer}
\end{brief}

Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). 
First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing.

One factor contributing to the success of neural networks is the ability to incorporate inductive biases that exploit the symmetry of data. Take convolutional neural networks (CNNs) for 2D images as an example: Patterns in images should be recognized regardless of their positions, which motivates the inductive bias of translational equivariance. As for atomistic graphs, where each atom has its coordinate in 3D Euclidean space, we consider inductive biases related to 3D Euclidean group E(3), which include equivariance to 3D translation, 3D rotation, and inversion. 


\subsection{EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Artical address: \href{https://arxiv.org/abs/2306.12059}{https://arxiv.org/abs/2306.12059}
    \item Code address: \href{https://github.com/atomicarchitects/equiformer\_v2}{https://github.com/atomicarchitects/equiformer\_v2}
\end{brief}




\chapter{Deep Molecular dynamics}







% \printbibliography[heading=bibintoc, title=\ebibname]


\end{document}