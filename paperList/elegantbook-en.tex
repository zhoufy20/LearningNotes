\documentclass[11pt]{elegantbook}
\setlength{\parindent}{0pt}

\title{Paper-List}
\subtitle{Documenting my literature reading journey}
\author{Feiyu Zhou}
\institute{State Key Laboratory of Strength and Vibration of Mechanical Structures,
\\ College of Aeronautics and Astronautics, Xi'an Jiaotong University}
\bioinfo{School:}{Xi'an Jiaotong University}
\date{\today}
\bioinfo{Mail:}{zhoufy.xjtu@gmail.cn}
\logo{xjtu.png}
\cover{cover.png}
\extrainfo{Literature review summary focus : what is studied, where is the innovation point, what is the research method, what is the conclusion.

Research entry point : which methodologies have been employed to address the fundamental issue, what are the strengths and limitations of the approach, and how can it be enhanced.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\frontmatter
\tableofcontents


\mainmatter

\chapter{Large Language Model}
\section{Equiformer}

\subsection{What is an equivariant neural network?}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{2006.10503 (arxiv.org)}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
    \end{brief}
Let V and W be sets, and f : V → W a function. If a group G acts on both V and W, and this action commutes with the function $f: f(x · v) = x · f(v)$ for all $v \in V, x \in G$, then we say that f is \textbf{G-equivariant}.The special case where G acts trivially on W is called \textbf{G-invariant}.



\subsection{Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds
}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author:  Nathaniel Thomas(Stanford University), Tess Smidt(University of California)
    \item Artical address: \href{https://arxiv.org/abs/1802.08219}{https://arxiv.org/abs/1802.08219}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}



\subsection{SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{https://arxiv.org/pdf/2006.10503}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}


Check out \href{https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/}{this work} by Alexandre Milesi et al. from Nvidia. They managed to speed up training of the SE(3)-Transformer by up to 21(!) times and reduced memory consumption by up to 43 times. \href{https://github.com/NVIDIA/DeepLearningExamples/tree/master/DGLPyTorch/DrugDiscovery/SE3Transformer}{Code here}.

The SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations.





\subsection{Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs}
\begin{brief}
    \item Journal: Arxiv
    \item Artical address: \href{https://arxiv.org/pdf/2206.11990}{https://arxiv.org/pdf/2206.11990}
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Code address: \href{https://github.com/atomicarchitects/equiformer}{https://github.com/atomicarchitects/equiformer}
\end{brief}

Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). 
First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing.

One factor contributing to the success of neural networks is the ability to incorporate inductive biases that exploit the symmetry of data. Take convolutional neural networks (CNNs) for 2D images as an example: Patterns in images should be recognized regardless of their positions, which motivates the inductive bias of translational equivariance. As for atomistic graphs, where each atom has its coordinate in 3D Euclidean space, we consider inductive biases related to 3D Euclidean group E(3), which include equivariance to 3D translation, 3D rotation, and inversion. 


\subsection{EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Artical address: \href{https://arxiv.org/abs/2306.12059}{https://arxiv.org/abs/2306.12059}
    \item Code address: \href{https://github.com/atomicarchitects/equiformer\_v2}{https://github.com/atomicarchitects/equiformer\_v2}
\end{brief}
















\chapter{Deep Molecular dynamics}

\section{molecular dynamics}
\subsection{Breaking the size limitation of nonadiabatic molecular dynamics in condensed matter systems with local descriptor machine learning}
\begin{brief}
    \item Journal: PNAS(Proceedings of the National Academy of Sciences)
    \item Lead Author: Andrey S. Vasenko(HSE University), Oleg V. Prezhdo(University of Southern California)
    \item Artical address: \href{https://www.pnas.org/doi/10.1073/pnas.2403497121}{https://www.pnas.org/doi/10.1073/pnas.2403497121}
    \item Code address: 
    \item KeyWords: NA-MD
\end{brief}

We have developed a fully ML approach to perform NA-MD simulations in large condensed-phase, nanoscale, and molecular systems. We have demonstrated that ML models based on local structural descriptors provide all the input needed to perform NAMD. The final NA-MD results are close to the ab initio calculations, performed using DFT and CPA, and the errors are within the accuracy of the current NA-MD methodologies and experimental uncertainties. Using the developed approach, we have elucidated the dependence of carrier trapping and recombination in defective MoS2 on defect concentration. This information is needed for design of optoelectronic and solar energy devices, and it is challenging to obtain from the conventional DFT/NA-MD investigations. The results show that the charge trapping rate decreases with decreasing defect concentration, while the charge recombination rate may or may not depend on defect concentration, conditional on whether the recombination occurs between free and trapped, or trapped charges, and on the relative concentrations of defects and carriers. The simulations have also shown that the dependence on defect concentration can vary with temperature, in particular, since delocalized shallow traps can become localized due to structure fluctuations. The developed approach provides an orders-of-magnitude speedup of NA-MD simulation compared to the traditional ab initio DFT simulation and can be accelerated further, presenting a valuable tool for modeling and characterization of excited state dynamics in realistic systems.


\section{DeePMD}
\subsection{MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields}
\begin{brief}
    \item Journal: Arxiv(Part of Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Main Conference Track)
    \item Lead Author: Ilyes Batatia(Engineering Laboratory, University of Cambridge, Cambridge, UK)
    \item Artical address: \href{https://arxiv.org/abs/2206.07697}{https://arxiv.org/abs/2206.07697}
    \item Code address: \href{https://github.com/ACEsuit/mace}{https://github.com/ACEsuit/mace}
\end{brief}
We summarise our main contributions as follows:
\begin{itemize}
    \item We introduce MACE, a novel architecture combining equivariant message passing with efficient many-body messages. The MACE models achieve state-of-the-art performance on challenging benchmark tests. They also display greater generalization capabilities over other approaches on extrapolation benchmarks.
    \item We demonstrate that many-body messages change the power of the empirical power-law of the learning curves. Furthermore, we show experimentally that the addition of equivariant messages only shifts the learning curves but does not change the power law when higher order messages are used.
    \item We show that MACE does not only outperform previous approaches in terms of accuracy but also does so while being significantly faster to train and evaluate than the previous most accurate models.
\end{itemize}

\subsection{DeePMD-kit: A deep learning package for many-body potential energy representation and molecular dynamics.}
\begin{brief}
    \item Journal: Computer Physics Communications 
    \item Lead Author: Han Wang(Institute of Applied Physics and Computational Mathematics, Fenghao East Road 2, Beijing 100094, PR China)
    \item Artical address: \href{https://www.sciencedirect.com/science/article/pii/S0010465518300882}{https://www.sciencedirect.com/science/article/pii/S0010465518300882}
    \item Code address: \href{https://github.com/deepmodeling}{https://github.com/deepmodeling}
\end{brief}


\section{Multi-Agent Intelligent}
\subsection{MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Markus J. Buehler(Laboratory for Atomistic and Molecular Mechanics (LAMM) Massachusetts Institute of Technology)
    \item Artical address: \href{https://arxiv.org/abs/2409.05556}{https://arxiv.org/abs/2409.05556}
    \item Code address: \href{https://github.com/lamm-mit/SciAgentsDiscovery}{https://github.com/lamm-mit/SciAgentsDiscovery}
    \item KeyWords: Point defects, 
\end{brief}
With the advent of
Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains
(e.g., failure mechanics-art). To achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset of
knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill
question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPT LLM foundation
model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various
language tasks, hypothesis generation, and connecting knowledge across disparate areas. 


\subsection{SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Markus J. Buehler(Laboratory for Atomistic and Molecular Mechanics (LAMM) Massachusetts Institute of Technology)
    \item Artical address: \href{https://arxiv.org/abs/2409.05556}{https://arxiv.org/abs/2409.05556}
    \item Code address: \href{https://github.com/lamm-mit/SciAgentsDiscovery}{https://github.com/lamm-mit/SciAgentsDiscovery}
    \item KeyWords: Point defects, 
\end{brief}
A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, we present SciAgents, an approach that leverages three core concepts: 
\begin{enumerate}
    \item the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts
    \item a suite of large language models(LLMs) and data retrieval tools
    \item multi-agent systems with in-situ learning capabilities.
\end{enumerate}

Building on these insights, our study introduces a method that synergizes the strengths of ontological knowledge graphs with the dynamic capabilities of LLM-based multi-agent systems, setting a robust foundation for enhancing graph reasoning and automating the scientific discovery process.




\section{CBPFNet}
\subsection{Identifying the ground state structures of point defects in solids}
\begin{brief}
    \item Journal: npj computational materials
    \item Lead Author: Irea Mosquera-Lois(Thomas Young Centre and Department of Chemistry, University College London, 20 Gordon Street, London WC1H 0AJ, UK.)
    \item Artical address: \href{https://www.nature.com/articles/s41524-023-00973-1}{https://www.nature.com/articles/s41524-023-00973-1}
    \item Code address: \href{https://github.com/SMTG-Bham/ShakeNBreak/tree/main}{https://github.com/SMTG-Bham/ShakeNBreak/tree/main}
    \item KeyWords: Point defects, 
\end{brief}







\chapter{2D materials and heterostructures}
\section{characterizing structure}





\section{predicting properties}






\section{inverse design}
\subsection{An invertible, invariant crystal representation for inverse design of solid-state materials using generative deep learning}
\begin{brief}
    \item Journal: Nature Communications
    \item Lead Author: Hang Xiao, Xi Chen(School of Interdisciplinary Studies, Lingnan University, Tuen Mun, Hong Kong SAR, China.)
    \item Artical address: \href{https://doi.org/10.1038/s41467-023-42870-7}{https://doi.org/10.1038/s41467-023-42870-7}
    \item Code address: 
\end{brief}
The past decade has witnessed rapid progress in deep learning for molecular
design, owing to the availability of invertible and invariant representations for
molecules such as simplified molecular-input line-entry system (SMILES),
which has powered cheminformatics since the late 1980s.

This is primarily due to, unlike molecular inverse design, the lack of an invertible crystal representation such as simplified molecular-input line-entry system (SMILES) that satisfies translational, rotational, and permutational invariances. To address this issue, we have developed a simplified line-input crystal-encoding system (SLICES), which is a string-based crystal representation that satisfies both invertibility and invariances.

The reconstruction of crystal structures from SLICES strings involves three steps: 
\begin{enumerate}
    \item initial structure generation with graph theory techniques
    \item optimization based on chemically meaningful geometry predicted with modified Geometry Frequency Noncovalent Force Field (GFN-FF)
    \item structural refinement using universal graph deep learning interatomic potential
\end{enumerate}



% \printbibliography[heading=bibintoc, title=\ebibname]


\end{document}