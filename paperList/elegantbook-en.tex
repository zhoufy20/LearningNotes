\documentclass[11pt]{elegantbook}
\setlength{\parindent}{0pt}

\title{Paper-List}
\subtitle{Documenting my literature reading journey}
\author{Feiyu Zhou}
\institute{State Key Laboratory of Strength and Vibration of Mechanical Structures,
\\ College of Aeronautics and Astronautics, Xi'an Jiaotong University}
\bioinfo{School:}{Xi'an Jiaotong University}
\date{\today}
\bioinfo{Mail:}{zhoufy.xjtu@gmail.cn}
\logo{xjtu.png}
\cover{cover.png}
\extrainfo{Literature review summary focus : what is studied, where is the innovation point, what is the research method, what is the conclusion.

Research entry point : which methodologies have been employed to address the fundamental issue, what are the strengths and limitations of the approach, and how can it be enhanced.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\frontmatter
\tableofcontents


\mainmatter

\chapter{Transformer}
\section{Equiformer}

\subsection{What is an equivariant neural network?}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{2006.10503 (arxiv.org)}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
    \end{brief}
Let V and W be sets, and f : V → W a function. If a group G acts on both V and W, and this action commutes with the function $f: f(x · v) = x · f(v)$ for all $v \in V, x \in G$, then we say that f is \textbf{G-equivariant}.The special case where G acts trivially on W is called \textbf{G-invariant}.



\subsection{Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds
}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author:  Nathaniel Thomas(Stanford University), Tess Smidt(University of California)
    \item Artical address: \href{https://arxiv.org/abs/1802.08219}{https://arxiv.org/abs/1802.08219}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}



\subsection{SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Fabian B. Fuchs(Bosch Center for Artificial Intelligence A2I Lab, Oxford University)
    \item Artical address: \href{https://arxiv.org/pdf/2006.10503}{https://arxiv.org/pdf/2006.10503}
    \item Code address: \href{https://github.com/FabianFuchsML/se3-transformer-public}{FabianFuchsML/se3-transformer-public: code for the SE3 Transformers paper}
\end{brief}


Check out \href{https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/}{this work} by Alexandre Milesi et al. from Nvidia. They managed to speed up training of the SE(3)-Transformer by up to 21(!) times and reduced memory consumption by up to 43 times. \href{https://github.com/NVIDIA/DeepLearningExamples/tree/master/DGLPyTorch/DrugDiscovery/SE3Transformer}{Code here}.

The SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations.





\subsection{Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs}
\begin{brief}
    \item Journal: Arxiv
    \item Artical address: \href{https://arxiv.org/pdf/2206.11990}{https://arxiv.org/pdf/2206.11990}
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Code address: \href{https://github.com/atomicarchitects/equiformer}{https://github.com/atomicarchitects/equiformer}
\end{brief}

Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). 
First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing.

One factor contributing to the success of neural networks is the ability to incorporate inductive biases that exploit the symmetry of data. Take convolutional neural networks (CNNs) for 2D images as an example: Patterns in images should be recognized regardless of their positions, which motivates the inductive bias of translational equivariance. As for atomistic graphs, where each atom has its coordinate in 3D Euclidean space, we consider inductive biases related to 3D Euclidean group E(3), which include equivariance to 3D translation, 3D rotation, and inversion. 


\subsection{EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations}
\begin{brief}
    \item Journal: Arxiv
    \item Lead Author: Yi-Lun Liao(Massachusetts Institute of Technology)
    \item Artical address: \href{https://arxiv.org/abs/2306.12059}{https://arxiv.org/abs/2306.12059}
    \item Code address: \href{https://github.com/atomicarchitects/equiformer\_v2}{https://github.com/atomicarchitects/equiformer\_v2}
\end{brief}


\section{Inverse design}
\subsection{An invertible, invariant crystal representation for inverse design of solid-state materials using generative deep learning}
\begin{brief}
    \item Journal: Nature Communications
    \item Lead Author: Hang Xiao, Xi Chen(School of Interdisciplinary Studies, Lingnan University, Tuen Mun, Hong Kong SAR, China.)
    \item Artical address: \href{https://doi.org/10.1038/s41467-023-42870-7}{https://doi.org/10.1038/s41467-023-42870-7}
    \item Code address: 
\end{brief}
The past decade has witnessed rapid progress in deep learning for molecular
design, owing to the availability of invertible and invariant representations for
molecules such as simplified molecular-input line-entry system (SMILES),
which has powered cheminformatics since the late 1980s.

This is primarily due to, unlike molecular inverse design, the lack of an invertible crystal representation such as simplified molecular-input line-entry system (SMILES) that satisfies translational, rotational, and permutational invariances. To address this issue, we have developed a simplified line-input crystal-encoding system (SLICES), which is a string-based crystal representation that satisfies both invertibility and invariances.

The reconstruction of crystal structures from SLICES strings involves three steps: 
\begin{enumerate}
    \item initial structure generation with graph theory techniques
    \item optimization based on chemically meaningful geometry predicted with modified Geometry Frequency Noncovalent Force Field (GFN-FF)
    \item structural refinement using universal graph deep learning interatomic potential
\end{enumerate}






\chapter{Deep Molecular dynamics}

\section{molecular dynamics}
\subsection{Breaking the size limitation of nonadiabatic molecular dynamics in condensed matter systems with local descriptor machine learning}
\begin{brief}
    \item Journal: PNAS(Proceedings of the National Academy of Sciences)
    \item Lead Author: Andrey S. Vasenko(HSE University), Oleg V. Prezhdo(University of Southern California)
    \item Artical address: \href{https://www.pnas.org/doi/10.1073/pnas.2403497121}{https://www.pnas.org/doi/10.1073/pnas.2403497121}
    \item Code address: 
    \item KeyWords: NA-MD
\end{brief}

We have developed a fully ML approach to perform NA-MD simulations in large condensed-phase, nanoscale, and molecular systems. We have demonstrated that ML models based on local structural descriptors provide all the input needed to perform NAMD. The final NA-MD results are close to the ab initio calculations, performed using DFT and CPA, and the errors are within the accuracy of the current NA-MD methodologies and experimental uncertainties. Using the developed approach, we have elucidated the dependence of carrier trapping and recombination in defective MoS2 on defect concentration. This information is needed for design of optoelectronic and solar energy devices, and it is challenging to obtain from the conventional DFT/NA-MD investigations. The results show that the charge trapping rate decreases with decreasing defect concentration, while the charge recombination rate may or may not depend on defect concentration, conditional on whether the recombination occurs between free and trapped, or trapped charges, and on the relative concentrations of defects and carriers. The simulations have also shown that the dependence on defect concentration can vary with temperature, in particular, since delocalized shallow traps can become localized due to structure fluctuations. The developed approach provides an orders-of-magnitude speedup of NA-MD simulation compared to the traditional ab initio DFT simulation and can be accelerated further, presenting a valuable tool for modeling and characterization of excited state dynamics in realistic systems.




% \printbibliography[heading=bibintoc, title=\ebibname]


\end{document}